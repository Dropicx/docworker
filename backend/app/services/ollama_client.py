import httpx
import json
import asyncio
import os
from typing import Optional, Dict, Any, AsyncGenerator
import re

class OllamaClient:
    
    def __init__(self, base_url: Optional[str] = None):
        # Container-zu-Container Kommunikation in Production
        if os.getenv("ENVIRONMENT") == "production":
            self.base_url = base_url or "http://ollama:11434"
        else:
            self.base_url = base_url or "http://localhost:11434"
            
        self.timeout = 300  # 5 Minuten Timeout
        
    async def check_connection(self) -> bool:
        """√úberpr√ºft Verbindung zu Ollama"""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.base_url}/api/version")
                return response.status_code == 200
        except Exception as e:
            print(f"‚ùå Ollama Verbindung fehlgeschlagen ({self.base_url}): {e}")
            return False
    
    async def list_models(self) -> list:
        """Listet verf√ºgbare Modelle auf"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [model["name"] for model in data.get("models", [])]
                return []
        except Exception as e:
            print(f"‚ùå Modell-Liste Fehler: {e}")
            return []
    
    async def translate_medical_text(
        self, 
        text: str, 
        document_type: str = "general",
        model: str = "llama3.2:latest"
    ) -> tuple[str, str, float]:
        """
        √úbersetzt medizinischen Text in einfache Sprache
        
        Returns:
            tuple[str, str, float]: (translated_text, detected_doc_type, confidence)
        """
        try:
            # Dokumenttyp erkennen
            detected_type = await self._detect_document_type(text)
            
            # Passenden Prompt ausw√§hlen
            prompt = self._get_translation_prompt(text, detected_type)
            
            # √úbersetzung durchf√ºhren
            translated_text = await self._generate_response(prompt, model)
            
            # Qualit√§t bewerten
            confidence = await self._evaluate_translation_quality(text, translated_text)
            
            return translated_text, detected_type, confidence
            
        except Exception as e:
            print(f"‚ùå √úbersetzung fehlgeschlagen: {e}")
            return f"Fehler bei der √úbersetzung: {str(e)}", "error", 0.0
    
    async def _detect_document_type(self, text: str) -> str:
        """Erkennt Art des medizinischen Dokuments"""
        text_lower = text.lower()
        
        # Schl√ºsselw√∂rter f√ºr verschiedene Dokumenttypen
        patterns = {
            "arztbrief": [
                "sehr geehrte", "liebe kollegin", "lieber kollege", 
                "entlassung", "aufnahme", "diagnose", "therapie",
                "empfehlung", "weiterbehandlung", "hochachtungsvoll"
            ],
            "laborbefund": [
                "laborwerte", "blutwerte", "referenzbereich", 
                "h√§matologie", "klinische chemie", "mg/dl", "mmol/l",
                "erh√∂ht", "erniedrigt", "normal"
            ],
            "radiologie": [
                "r√∂ntgen", "ct", "mrt", "ultraschall", "befund",
                "darstellung", "kontrastmittel", "auff√§llig",
                "unauff√§llig", "verdacht"
            ],
            "pathologie": [
                "histologie", "biopsie", "gewebeprobe", "tumor",
                "maligne", "benigne", "metastase", "grading"
            ]
        }
        
        scores = {}
        for doc_type, keywords in patterns.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            scores[doc_type] = score
        
        # H√∂chsten Score finden
        if scores:
            detected = max(scores, key=scores.get)
            if scores[detected] >= 2:  # Mindestens 2 Treffer
                return detected
        
        return "allgemein"
    
    def _get_translation_prompt(self, text: str, doc_type: str) -> str:
        """Erstellt optimierten Prompt basierend auf Dokumenttyp"""
        
        base_instruction = """Du bist ein medizinischer √úbersetzer, der komplexe medizinische Texte in einfache, verst√§ndliche Sprache f√ºr Patienten √ºbersetzt.

WICHTIGE REGELN:
- √úbersetze ALLE medizinischen Fachbegriffe in einfache deutsche Sprache
- Erkl√§re Diagnosen und Befunde so, dass sie jeder verstehen kann
- Behalte wichtige Informationen bei, aber mache sie verst√§ndlich
- Verwende eine beruhigende, positive Sprache
- Strukturiere den Text √ºbersichtlich
- F√ºge bei Bedarf kurze Erkl√§rungen hinzu"""
        
        specific_instructions = {
            "arztbrief": """
SPEZIELLE ANWEISUNGEN F√úR ARZTBRIEFE:
- Beginne mit einer freundlichen Zusammenfassung
- Erkl√§re den Grund des Arztbesuches/Krankenhausaufenthaltes  
- √úbersetze alle Diagnosen in verst√§ndliche Begriffe
- Erkl√§re empfohlene Behandlungen und deren Zweck
- Erw√§hne wichtige n√§chste Schritte""",
            
            "laborbefund": """
SPEZIELLE ANWEISUNGEN F√úR LABORBEFUNDE:
- Erkl√§re, was die einzelnen Werte bedeuten
- Teile mit, ob Werte normal, erh√∂ht oder erniedrigt sind
- Erkl√§re m√∂gliche Ursachen f√ºr auff√§llige Werte
- Beruhige bei normalen Werten
- Erw√§hne n√§chste Schritte bei auff√§lligen Befunden""",
            
            "radiologie": """
SPEZIELLE ANWEISUNGEN F√úR RADIOLOGIE-BEFUNDE:
- Erkl√§re, welche Untersuchung durchgef√ºhrt wurde
- √úbersetze anatomische Begriffe in einfache Sprache
- Erkl√§re Befunde verst√§ndlich (normal/auff√§llig)
- Beruhige bei unauff√§lligen Befunden
- Erkl√§re weitere Schritte bei auff√§lligen Befunden""",
            
            "pathologie": """
SPEZIELLE ANWEISUNGEN F√úR PATHOLOGIE-BEFUNDE:
- Erkl√§re vorsichtig und einf√ºhlsam
- √úbersetze alle Fachbegriffe
- Erkl√§re, was die Untersuchung ergeben hat
- Verwende beruhigende Sprache
- Erw√§hne n√§chste Behandlungsschritte"""
        }
        
        instruction = base_instruction
        if doc_type in specific_instructions:
            instruction += specific_instructions[doc_type]
        
        return f"""{instruction}

ORIGINAL MEDIZINISCHER TEXT:
{text}

EINFACHE √úBERSETZUNG:"""
    
    async def _generate_response(self, prompt: str, model: str) -> str:
        """Generiert Antwort von Ollama"""
        try:
            # Erst versuchen, verf√ºgbare Modelle zu laden, falls das angegebene Modell nicht existiert
            if model not in await self.list_models():
                print(f"‚ö†Ô∏è Modell {model} nicht verf√ºgbar, verwende Fallback...")
                available_models = await self.list_models()
                
                # Fallback-Logik: Bevorzuge Llama-Modelle, dann andere
                fallback_models = [
                    "llama3.2:latest", "llama3.1", "mistral:7b", 
                    "deepseek-r1:7b", "gemma3:27b"
                ]
                
                for fallback in fallback_models:
                    if fallback in available_models:
                        model = fallback
                        print(f"‚úÖ Verwende Fallback-Modell: {model}")
                        break
                else:
                    # Wenn kein Fallback gefunden, nimm das erste verf√ºgbare Modell
                    if available_models:
                        model = available_models[0]
                        print(f"‚úÖ Verwende erstes verf√ºgbares Modell: {model}")
                    else:
                        return "Fehler: Keine Modelle verf√ºgbar"
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,  # Niedrig f√ºr konsistente medizinische √úbersetzungen
                        "top_p": 0.9,
                        "top_k": 40,
                        "num_predict": 2000  # L√§ngere Antworten f√ºr ausf√ºhrliche Erkl√§rungen
                    }
                }
                
                print(f"ü§ñ Generiere mit Modell: {model}")
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", "Keine Antwort erhalten").strip()
                else:
                    print(f"‚ùå Ollama API Error: {response.status_code} - {response.text}")
                    return f"Fehler bei der Ollama-Anfrage: {response.status_code}"
                    
        except Exception as e:
            print(f"‚ùå Ollama-Generation Fehler: {e}")
            return f"Fehler bei der KI-√úbersetzung: {str(e)}"
    
    async def _evaluate_translation_quality(self, original: str, translated: str) -> float:
        """Bewertet Qualit√§t der √úbersetzung"""
        if not translated or translated.startswith("Fehler"):
            return 0.0
        
        confidence = 0.5  # Basis-Vertrauen
        
        # L√§nge der √úbersetzung
        if len(translated) > 100:
            confidence += 0.1
        if len(translated) > 500:
            confidence += 0.1
        
        # Verh√§ltnis Original zu √úbersetzung (sollte nicht zu stark abweichen)
        length_ratio = len(translated) / max(len(original), 1)
        if 0.5 <= length_ratio <= 2.0:
            confidence += 0.1
        
        # Einfache Sprache Indikatoren
        simple_indicators = [
            "das bedeutet", "einfach gesagt", "vereinfacht", 
            "das hei√üt", "mit anderen worten"
        ]
        translated_lower = translated.lower()
        found_indicators = sum(1 for indicator in simple_indicators if indicator in translated_lower)
        confidence += min(found_indicators * 0.05, 0.2)
        
        # Medizinische Fachbegriffe sollten reduziert sein
        medical_terms = [
            "pathologie", "histologie", "maligne", "benigne",
            "√§tiologie", "therapeutisch", "diagnostisch"
        ]
        original_terms = sum(1 for term in medical_terms if term in original.lower())
        translated_terms = sum(1 for term in medical_terms if term in translated_lower)
        
        if original_terms > 0:
            reduction_rate = 1 - (translated_terms / original_terms)
            confidence += reduction_rate * 0.1
        
        return min(confidence, 1.0)
    
    async def generate_streaming(
        self, 
        prompt: str, 
        model: str = "llama3.2:latest"
    ) -> AsyncGenerator[str, None]:
        """Streaming-Generation f√ºr Live-Updates"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "top_k": 40
                    }
                }
                
                async with client.stream(
                    "POST", 
                    f"{self.base_url}/api/generate",
                    json=payload
                ) as response:
                    async for chunk in response.aiter_lines():
                        if chunk:
                            try:
                                data = json.loads(chunk)
                                if "response" in data:
                                    yield data["response"]
                                if data.get("done", False):
                                    break
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            yield f"Streaming-Fehler: {str(e)}" 