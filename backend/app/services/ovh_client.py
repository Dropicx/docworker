import base64
from collections.abc import AsyncGenerator
from io import BytesIO
import logging
from typing import Any

import httpx
from openai import AsyncOpenAI
from PIL import Image

from app.core.config import settings

# âš¡ NOTE: PII removal now happens in worker (OptimizedPrivacyFilter)
# This service receives already-cleaned text from the worker
# No need to import privacy filters here anymore

# Setup logger
logger = logging.getLogger(__name__)


class OVHClient:
    """OVH AI Endpoints client for medical text processing.

    This client provides a comprehensive interface to OVH's AI infrastructure,
    supporting text-based LLM operations for medical document translation,
    processing, and streaming responses.

    The client uses different models optimized for specific tasks:
    - Main Model (Llama 3.3 70B): High-quality translation and processing
    - Preprocessing Model (Mistral Nemo): Fast routine tasks

    Note: OCR is now handled by OCREngineManager (Mistral OCR + PaddleOCR Hetzner)

    Attributes:
        access_token (str): OVH API access token from settings
        base_url (str): Base URL for OVH AI Endpoints
        main_model (str): Primary LLM model for high-quality tasks
        preprocessing_model (str): Fast model for routine operations
        translation_model (str): Model for language translation
        client (AsyncOpenAI): OpenAI-compatible client for text models
        timeout (int): Request timeout in seconds

    Example:
        >>> client = OVHClient()
        >>> result = await client.translate_medical_document(
        ...     text="Medical text...",
        ...     document_type="ARZTBRIEF"
        ... )

    Note:
        All methods that call AI models return token usage information for
        cost tracking. The client automatically selects the optimal model
        based on task complexity.
    """

    def __init__(self) -> None:
        # Load configuration from settings
        self.access_token = settings.ovh_api_token
        self.base_url = settings.ovh_ai_base_url

        # Different models for different tasks
        self.main_model = settings.ovh_main_model
        self.preprocessing_model = settings.ovh_preprocessing_model
        self.translation_model = settings.ovh_translation_model

        # Define which prompt types should use fast model for speed optimization
        self.fast_model_prompt_types = {
            "preprocessing_prompt",
            "language_translation_prompt",
            "grammar_check_prompt",
            "final_check_prompt",
            "formatting_prompt",
        }

        # âš¡ NOTE: PII filtering now happens in worker before pipeline execution
        # No privacy filter needed here - text arrives pre-cleaned
        # âš¡ NOTE: OCR is now handled by OCREngineManager (Mistral OCR + PaddleOCR)

        # Debug logging for configuration (only if debug enabled)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("ðŸ” OVH Client Initialization:")
            logger.debug(f"   - Access Token: {'âœ… Set' if self.access_token else 'âŒ NOT SET'}")
            logger.debug(
                f"   - Token Length: {len(self.access_token) if self.access_token else 0} chars"
            )
            logger.debug(f"   - Base URL: {self.base_url}")
            logger.debug(f"   - Main Model: {self.main_model}")
            logger.debug(f"   - USE_OVH_ONLY: {settings.use_ovh_only}")

        if not self.access_token:
            logger.warning("âš ï¸ OVH_AI_ENDPOINTS_ACCESS_TOKEN not set - API calls will fail!")
            logger.warning("   Please set the following environment variables in Railway:")
            logger.warning("   - OVH_AI_ENDPOINTS_ACCESS_TOKEN=your-token-here")
            logger.warning("   - OVH_AI_BASE_URL=https://oai.endpoints.kepler.ai.cloud.ovh.net/v1")

        # Initialize OpenAI client for OVH (use dummy key to prevent initialization errors)
        try:
            self.client = AsyncOpenAI(
                base_url=self.base_url,
                api_key=self.access_token or "dummy-key-not-set",  # Use dummy key if not set
            )
        except Exception as e:
            logger.error(f"Failed to initialize OVH client: {e}")
            self.client = None

        # Alternative HTTP client for direct API calls
        self.timeout = settings.ai_timeout_seconds

    async def check_connection(self) -> tuple[bool, str]:
        """Verify connectivity and authentication with OVH AI Endpoints.

        Performs a simple test API call to validate that the client is properly
        configured and can communicate with OVH's AI infrastructure.

        Returns:
            tuple[bool, str]: A tuple containing:
                - bool: True if connection successful, False otherwise
                - str: Success message or detailed error description

        Example:
            >>> client = OVHClient()
            >>> success, message = await client.check_connection()
            >>> if success:
            ...     print(f"Connected: {message}")
            ... else:
            ...     print(f"Connection failed: {message}")

        Note:
            Provides specific error guidance for common issues:
            - 401: Invalid or expired API token
            - 404: Model not found or unavailable
            - Timeout: Network connectivity issues
        """
        if not self.access_token:
            error = (
                "OVH API token not configured - OVH_AI_ENDPOINTS_ACCESS_TOKEN is empty or not set"
            )
            logger.error(f"âŒ {error}")
            logger.error("   Please ensure the environment variable is set in Railway")
            return False, error

        if not self.client:
            error = "OVH client not initialized"
            logger.error(f"âŒ {error}")
            return False, error

        try:
            logger.debug(f"ðŸ”„ Testing OVH connection to {self.base_url}")
            logger.debug(f"   Using model: {self.main_model}")
            logger.debug(
                f"   Token (last 8 chars): ...{self.access_token[-8:] if self.access_token else 'NOT SET'}"
            )

            # Try a simple completion to test connection
            response = await self.client.chat.completions.create(
                model=self.main_model,
                messages=[{"role": "user", "content": "Say 'OK' if you can read this"}],
                max_tokens=10,
                temperature=0,
            )

            if response and response.choices:
                logger.info("âœ… OVH AI Endpoints connection successful")
                logger.info(f"   Response: {response.choices[0].message.content[:50]}")
                return True, "Connection successful"
            error = "Empty response from OVH API"
            logger.error(f"âŒ {error}")
            return False, error

        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ OVH AI Endpoints connection failed: {error_msg}")

            # Provide specific guidance based on error
            if "401" in error_msg or "unauthorized" in error_msg.lower():
                error = f"Invalid API token (401 Unauthorized). Token last 8 chars: ...{self.access_token[-8:] if self.access_token else 'NOT SET'}"
                logger.error(f"   â†’ {error}")
            elif "404" in error_msg:
                error = f"Model '{self.main_model}' not found (404). Available models may differ."
                logger.error(f"   â†’ {error}")
            elif "connection" in error_msg.lower() or "timeout" in error_msg.lower():
                error = f"Cannot reach {self.base_url} (Connection/Timeout error)"
                logger.error(f"   â†’ {error}")
            else:
                error = f"Unexpected error: {error_msg[:200]}"
                logger.error(f"   â†’ {error}")

            return False, error

    def should_use_fast_model(self, prompt_type: str = None, task_description: str = None) -> bool:
        """
        Determine whether to use fast model based on prompt type or task description.
        Fast model is used for routine tasks to improve speed.
        """
        if prompt_type and prompt_type in self.fast_model_prompt_types:
            return True

        # Additional heuristics based on task description
        if task_description:
            task_lower = task_description.lower()
            speed_optimized_keywords = [
                "grammar",
                "formatting",
                "format",
                "structure",
                "layout",
                "final check",
                "validation",
                "template",
                "language_translation",
            ]
            return any(keyword in task_lower for keyword in speed_optimized_keywords)

        return False

    async def process_medical_text_with_prompt(
        self,
        full_prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 4000,
        use_fast_model: bool = False,
    ) -> dict[str, Any]:
        """Process medical text using AI with intelligent model selection.

        Sends a complete prompt to OVH AI and returns the processed result along
        with detailed token usage for cost tracking. Automatically selects between
        high-quality (Llama 3.3 70B) and fast (Mistral Nemo) models based on task.

        Args:
            full_prompt: Complete prompt including instructions and input text
            temperature: Sampling temperature (0.0-1.0). Lower = more deterministic.
                Default 0.3 for medical accuracy.
            max_tokens: Maximum tokens to generate in response. Default 4000.
            use_fast_model: If True, uses Mistral Nemo for speed. If False, uses
                Llama 3.3 70B for quality. Default False.

        Returns:
            dict[str, Any]: Processing result containing:
                - text (str): The AI-generated response
                - input_tokens (int): Number of tokens in the prompt
                - output_tokens (int): Number of tokens in the response
                - total_tokens (int): Sum of input and output tokens
                - model (str): Name of the model that was used

        Example:
            >>> client = OVHClient()
            >>> prompt = "Translate this medical text: {text}"
            >>> result = await client.process_medical_text_with_prompt(
            ...     full_prompt=prompt,
            ...     temperature=0.3,
            ...     use_fast_model=False
            ... )
            >>> print(f"Used {result['total_tokens']} tokens")
            >>> print(f"Response: {result['text']}")

        Note:
            - Fast model (Mistral Nemo): 50-100ms faster, suitable for formatting/grammar
            - High-quality model (Llama 3.3 70B): Best for translation and medical content
            - Token usage is returned for cost tracking via AICostTracker
        """
        if not self.access_token:
            logger.error("âŒ OVH API token not configured")
            return {
                "text": "Error: OVH API token not configured. Please set OVH_AI_ENDPOINTS_ACCESS_TOKEN in .env",
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "model": None,
            }

        # Choose model based on task type
        model_to_use = self.preprocessing_model if use_fast_model else self.main_model
        model_type = "fast" if use_fast_model else "high-quality"

        try:
            logger.debug(f"ðŸš€ Processing with OVH {model_to_use} ({model_type})")

            # Use simple user message with the full prompt
            messages = [{"role": "user", "content": full_prompt}]

            # Make the API call using OpenAI client
            response = await self.client.chat.completions.create(
                model=model_to_use,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.9,
            )

            result = response.choices[0].message.content

            # âœ¨ NEW: Extract token usage from response
            usage = getattr(response, "usage", None)
            input_tokens = getattr(usage, "prompt_tokens", 0) if usage else 0
            output_tokens = getattr(usage, "completion_tokens", 0) if usage else 0
            total_tokens = getattr(usage, "total_tokens", 0) if usage else 0

            if not usage:
                logger.warning("âš ï¸ API response has no usage data")

            logger.debug(
                f"âœ… OVH processing successful with {model_to_use} ({model_type}) - {total_tokens} tokens"
            )

            return {
                "text": result.strip(),
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "model": model_to_use,
            }

        except Exception as e:
            logger.error(f"âŒ OVH API error: {e}")
            return {
                "text": f"Error processing with OVH API: {str(e)}",
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0,
                "model": model_to_use,
            }

    async def process_prompt_with_optimization(
        self,
        full_prompt: str,
        prompt_type: str = None,
        temperature: float = 0.3,
        max_tokens: int = 4000,
    ) -> str:
        """
        Process a prompt with automatic model optimization based on prompt type.
        This is the recommended method for processing prompts in the pipeline.
        """
        use_fast = self.should_use_fast_model(prompt_type=prompt_type)

        logger.info(
            f"ðŸ”„ Processing prompt type '{prompt_type}' with {'fast' if use_fast else 'high-quality'} model"
        )

        return await self.process_medical_text_with_prompt(
            full_prompt=full_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            use_fast_model=use_fast,
        )

    async def process_medical_text(
        self,
        text: str,
        instruction: str = "Process this medical text",
        temperature: float = 0.3,
        max_tokens: int = 4000,
    ) -> str:
        """
        Process medical text using Meta-Llama-3.3-70B-Instruct

        Args:
            text: The medical text to process
            instruction: Processing instruction
            temperature: Model temperature (0-1)
            max_tokens: Maximum tokens to generate

        Returns:
            Processed text from the model
        """
        if not self.access_token:
            logger.error("âŒ OVH API token not configured")
            return "Error: OVH API token not configured. Please set OVH_AI_ENDPOINTS_ACCESS_TOKEN in .env"

        try:
            logger.debug(f"ðŸš€ Processing with OVH {self.main_model}")

            # Prepare the message
            messages = [
                {
                    "role": "system",
                    "content": "Du bist ein hochspezialisierter medizinischer Textverarbeiter. Befolge die Anweisungen prÃ¤zise. Antworte IMMER in der gleichen Sprache wie der Eingabetext.",
                },
                {"role": "user", "content": f"{instruction}\n\nText to process:\n{text}"},
            ]

            # Make the API call using OpenAI client
            response = await self.client.chat.completions.create(
                model=self.main_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.9,
            )

            result = response.choices[0].message.content
            logger.debug("âœ… OVH processing successful")
            return result.strip()

        except Exception as e:
            logger.error(f"âŒ OVH API error: {e}")
            return f"Error processing with OVH API: {str(e)}"

    async def preprocess_medical_text(
        self, text: str, temperature: float = 0.3, max_tokens: int = 4000
    ) -> str:
        """
        Preprocess medical text - first removes PII locally, then optionally uses OVH
        """
        logger.info(f"ðŸ“„ PREPROCESSING PIPELINE STARTED ({len(text)} characters)")

        # âš¡ PII REMOVAL NOW HAPPENS IN WORKER (before pipeline)
        # This preprocessing step no longer needs to remove PII
        # Text arrives here already cleaned by OptimizedPrivacyFilter in worker
        logger.info("â„¹ï¸  [2/3] Text already PII-filtered by worker (OptimizedPrivacyFilter)")
        cleaned_text = text

        # SCHRITT 2: Optional zusÃ¤tzliche Bereinigung mit OVH (wenn API verfÃ¼gbar)
        # Dies ist jetzt optional - wenn OVH nicht verfÃ¼gbar, verwenden wir nur lokale Bereinigung
        if not self.access_token:
            logger.debug("â„¹ï¸ OVH API not configured, using local PII removal only")
            return cleaned_text  # Return locally cleaned text

        try:
            logger.debug(f"ðŸ”§ Additional preprocessing with OVH {self.preprocessing_model}")

            preprocess_prompt = """Du bist ein medizinischer Dokumentenbereiniger fÃ¼r Datenschutz und Ãœbersichtlichkeit.

ðŸš¨ KRITISCHE REGEL: BEHALTE ABSOLUT ALLE MEDIZINISCHEN INFORMATIONEN!

ENTFERNE NUR (komplett lÃ¶schen):
- Patientennamen und Patientenadressen
- Geburtsdaten von Patienten (ABER: Untersuchungsdaten mÃ¼ssen bleiben!)
- Arztnamen und Unterschriften (ABER: Fachabteilungen bleiben!)
- Versicherungsnummern, Patientennummern
- Private Telefonnummern und E-Mails
- BriefkÃ¶pfe, Logos, reine Formatierungszeichen
- Seitenzahlen, Kopf-/FuÃŸzeilen
- GruÃŸformeln (z.B. "Mit freundlichen GrÃ¼ÃŸen", "Sehr geehrte")
- Anreden und Verabschiedungen

âš ï¸ MUSS UNBEDINGT BLEIBEN - NIEMALS LÃ–SCHEN:
âœ… ALLE Laborwerte (auch in Tabellen, Listen oder ANHÃ„NGEN!)
âœ… ALLE Blutwerte, Urinwerte, etc.
âœ… ALLE Messwerte und Zahlen mit medizinischer Bedeutung
âœ… ALLE Referenzbereiche und Normwerte
âœ… ALLE AnhÃ¤nge und deren KOMPLETTE Inhalte
âœ… ALLE Verweise auf AnhÃ¤nge (z.B. "siehe Anhang", "Laborwerte im Anhang")
âœ… KOMPLETTE AnhÃ¤nge mit Laborwerten, auch wenn sie am Ende stehen
âœ… ALLE Diagnosen und Befunde
âœ… ALLE Medikamente und Dosierungen
âœ… ALLE medizinischen Daten und Termine
âœ… ALLE Untersuchungsergebnisse
âœ… Krankenhaus-/Abteilungsnamen
âœ… Der KOMPLETTE medizinische Inhalt
âœ… Medizinische Codes (ICD, OPS, etc.)

ðŸ”´ SPEZIALREGEL FÃœR ANHÃ„NGE:
Wenn "siehe Anhang" oder "Laborwerte im Anhang" erwÃ¤hnt wird:
â†’ BEHALTE den Verweis UND den kompletten Anhang-Inhalt!
â†’ Auch wenn der Anhang am Ende steht, BEHALTE IHN KOMPLETT!
â†’ Entferne NUR Patientendaten aus dem Anhang, NICHT die Werte!

WICHTIG: Wenn du dir unsicher bist, BEHALTE die Information!

ORIGINALTEXT:
{text}

BEREINIGTER TEXT (nur medizinische Inhalte):"""

            full_prompt = preprocess_prompt.format(text=cleaned_text)

            # Use preprocessing model
            messages = [{"role": "user", "content": full_prompt}]

            response = await self.client.chat.completions.create(
                model=self.preprocessing_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.9,
            )

            result = response.choices[0].message.content

            logger.info(f"âœ… OVH preprocessing completed: {len(result)} characters (reduced by {len(text) - len(result)})")

            # Clean up formatting
            import re

            result = re.sub(r"^\s*\d+[.)]\s*([â€¢\-\*])", r"\1", result, flags=re.MULTILINE)
            result = re.sub(r"^([â€¢\-\*])\s*[â€¢\-\*]+\s*", r"\1 ", result, flags=re.MULTILINE)
            result = re.sub(r"([â€¢\-\*])\s*\1+", r"\1", result)

            return result.strip() if result else text

        except Exception as e:
            logger.error(f"âŒ OVH preprocessing error: {e}")
            return text  # Return original text on error

    async def translate_to_language(
        self,
        simplified_text: str,
        target_language: str,
        temperature: float = 0.3,
        max_tokens: int = 4000,
        custom_prompt: str | None = None,
    ) -> tuple[str, float]:
        """Translate simplified medical text to target language with quality scoring.

        Translates patient-friendly medical text to another language while preserving
        formatting, medical terms, and numerical values. Automatically evaluates
        translation quality based on multiple indicators.

        Args:
            simplified_text: German medical text already simplified for patients
            target_language: Target language code (e.g., "English", "French", "Spanish")
            temperature: AI temperature (0.0-1.0). Default 0.3 for accuracy.
            max_tokens: Maximum response tokens. Default 4000.
            custom_prompt: Optional custom translation prompt. If None, uses
                default prompt with strict formatting rules.

        Returns:
            tuple[str, float]: A tuple containing:
                - str: Translated text with preserved formatting
                - float: Translation quality confidence (0.0-1.0)

        Example:
            >>> client = OVHClient()
            >>> de_text = "Ihr Blutdruck ist erhÃ¶ht (145/90 mmHg)."
            >>> en_text, confidence = await client.translate_to_language(
            ...     simplified_text=de_text,
            ...     target_language="English"
            ... )
            >>> print(f"Translation ({confidence:.0%} confidence): {en_text}")
            >>> # Output: "Your blood pressure is elevated (145/90 mmHg)."

        Note:
            **Quality Indicators**:
            - Text length comparison (0.7-1.5 ratio expected)
            - Symbol preservation (bullets, arrows, emojis)
            - Structure preservation (formatting, line breaks)

            **Translation Rules**:
            - Numbers and units preserved exactly
            - Medical symbols (â€¢, â†’, ##) unchanged
            - Formatting (line breaks, indentation) maintained
        """
        if not self.access_token:
            logger.error("âŒ OVH API token not configured")
            return simplified_text, 0.0

        try:
            logger.debug(f"ðŸŒ Translating to {target_language} with OVH {self.translation_model}")

            if custom_prompt:
                # Use custom prompt and replace placeholders
                translation_prompt = custom_prompt.replace("{language}", target_language).replace(
                    "{text}", simplified_text
                )
                logger.info("ðŸ“ Using custom language translation prompt")
            else:
                translation_prompt = f"""Ãœbersetze den folgenden Text EXAKT in {target_language}.

STRIKTE REGELN:
1. NUR Ã¼bersetzen - KEINE ZusÃ¤tze, ErklÃ¤rungen oder Kommentare
2. EXAKTE Formatierung beibehalten - jede Zeile, jeder Absatz, jedes Symbol
3. Alle Symbole (â€¢, â†’, ##, ðŸ“Š, etc.) UNVERÃ„NDERT lassen
4. Zahlen und Einheiten (mg, ml, mmHg) NICHT Ã¤ndern
5. Bei unÃ¼bersetzbaren Begriffen das Original verwenden

TEXT ZUM ÃœBERSETZEN:
{simplified_text}

ÃœBERSETZUNG:"""
                logger.info("ðŸ“ Using default language translation prompt")

            messages = [{"role": "user", "content": translation_prompt}]

            response = await self.client.chat.completions.create(
                model=self.translation_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=0.9,
            )

            result = response.choices[0].message.content
            logger.debug("âœ… OVH language translation successful")

            # Improve formatting for bullet points and arrows
            result = self._improve_formatting(result)

            # Evaluate quality
            confidence = self._evaluate_language_translation_quality(simplified_text, result)

            return result.strip(), confidence

        except Exception as e:
            logger.error(f"âŒ OVH language translation error: {e}")
            return simplified_text, 0.0

    def _evaluate_language_translation_quality(self, original: str, translated: str) -> float:
        """
        Evaluate the quality of language translation
        """
        if not translated or translated.startswith("Error"):
            return 0.0

        confidence = 0.6  # Base confidence for OVH model

        # Length check
        if len(translated) > 50:
            confidence += 0.1

        # Ratio check
        length_ratio = len(translated) / max(len(original), 1)
        if 0.7 <= length_ratio <= 1.5:
            confidence += 0.1

        # Structure preservation (emojis)
        import re

        emoji_pattern = (
            r"[ðŸ˜€-ðŸ¿¿]|[\U0001F300-\U0001F5FF]|[\U0001F600-\U0001F64F]|[\U0001F680-\U0001F6FF]"
        )
        original_emojis = len(re.findall(emoji_pattern, original))
        translated_emojis = len(re.findall(emoji_pattern, translated))

        if original_emojis > 0:
            emoji_retention = min(translated_emojis / original_emojis, 1.0)
            confidence += emoji_retention * 0.1

        return min(confidence, 1.0)

    async def generate_streaming(
        self, prompt: str, temperature: float = 0.3, max_tokens: int = 4000
    ) -> AsyncGenerator[str, None]:
        """
        Generate streaming response from OVH API
        """
        if not self.access_token:
            yield "Error: OVH API token not configured"
            return

        try:
            stream = await self.client.chat.completions.create(
                model=self.main_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True,
            )

            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            logger.error(f"âŒ OVH streaming error: {e}")
            yield f"Streaming error: {str(e)}"

    async def translate_medical_document(
        self, text: str, document_type: str = "universal", custom_prompts: Any | None = None
    ) -> tuple[str, str, float, str]:
        """
        Main processing using OVH Meta-Llama-3.3-70B for medical document translation

        Returns:
            tuple[str, str, float, str]: (translated_text, doc_type, confidence, cleaned_original)
        """
        try:
            logger.info(f"ðŸŒ TRANSLATION PIPELINE STARTED ({len(text)} characters)")

            # Create the comprehensive instruction for medical translation (in German)
            if custom_prompts and hasattr(custom_prompts, "translation_prompt"):
                instruction = custom_prompts.translation_prompt
                logger.info(f"ðŸ“ Using custom translation prompt for {document_type}")
            else:
                instruction = self._get_medical_translation_instruction()
                logger.info(f"ðŸ“ Using default translation prompt for {document_type}")

            # Format the complete prompt
            full_prompt = f"""{instruction}

ORIGINAL MEDIZINISCHER TEXT:
{text}

ÃœBERSETZUNG IN EINFACHER SPRACHE:"""

            # Process with OVH API using the formatted prompt (main translation - use high-quality model)
            translated_text = await self.process_medical_text_with_prompt(
                full_prompt=full_prompt,
                temperature=0.3,
                max_tokens=4000,
                use_fast_model=False,  # Main translation needs quality
            )

            # Improve formatting for bullet points and arrows
            translated_text = self._improve_formatting(translated_text)

            # Evaluate quality
            confidence = self._evaluate_translation_quality(text, translated_text)

            logger.info(f"âœ… TRANSLATION COMPLETED: {len(translated_text)} characters, confidence: {confidence:.2%}")

            return translated_text, document_type, confidence, text

        except Exception as e:
            logger.error(f"âŒ OVH translation failed: {e}")
            return f"Translation error: {str(e)}", "error", 0.0, text

    def _get_medical_translation_instruction(self) -> str:
        """Get the comprehensive medical translation instruction"""

        base_instruction = """Du bist ein hochspezialisierter medizinischer Ãœbersetzer. Deine Aufgabe ist es, medizinische Dokumente vollstÃ¤ndig und prÃ¤zise in patientenfreundliche Sprache zu Ã¼bersetzen.

KRITISCHE ANTI-HALLUZINATIONS-REGELN:
â›” FÃœGE NICHTS HINZU was nicht explizit im Dokument steht
â›” KEINE Vermutungen, Annahmen oder "kÃ¶nnte sein" Aussagen
â›” KEINE allgemeinen medizinischen RatschlÃ¤ge die nicht im Text stehen
â›” KEINE zusÃ¤tzlichen ErklÃ¤rungen auÃŸer direkte Ãœbersetzung von Fachbegriffen
â›” KEINE Verweise auf AnhÃ¤nge ("siehe Anhang", "weitere Werte im Anhang") wenn diese nicht explizit im Text erwÃ¤hnt werden
â›” ERFINDE KEINE zusÃ¤tzlichen Informationen die nicht da sind
â›” KEINE Meta-Kommentare wie "Alle Angaben entsprechen dem Originaltext" oder "Diese Information stammt aus dem Dokument"
â›” KEINE Hinweise darauf, dass du Ã¼bersetzt oder dass dies eine Ãœbersetzung ist
âœ… Ãœbersetze NUR was wÃ¶rtlich im Dokument steht
âœ… Lasse KEINE medizinische Information weg
âœ… ErklÃ¤re Fachbegriffe kurz in Klammern (nur Definition, keine Zusatzinfos)
âœ… Spreche den Patienten DIREKT an (nutze "Sie", "Ihr", "Ihnen")
âœ… Bei Unklarheiten: markiere mit [unklar] statt zu interpretieren
âœ… KEINE Behandlungsempfehlungen die nicht im Original stehen
âœ… ERKLÃ„RE IMMER medizinische Codes (ICD, OPS, DRG, etc.) - nie nur auflisten!


SPRACHLICHE RICHTLINIEN:

VERWENDE:
Kurze HauptsÃ¤tze (maximal 15-20 WÃ¶rter)
Aktive Formulierungen ("Der Arzt untersucht" statt "Es wird untersucht")
Konkrete Begriffe ("Blutdruck messen" statt "Blutdruckkontrolle durchfÃ¼hren")
Alltagssprache ("Herz" zusÃ¤tzlich zu "kardial")
Vergleiche aus dem Alltag (z.B. "groÃŸ wie eine Walnuss")
Zahlen ausschreiben wenn verstÃ¤ndlicher ("zwei Mal tÃ¤glich" statt "2x tÃ¤gl.")
Direkte Ansprache ("Sie waren", "Ihr Blutdruck", "Sie sollen")

VERMEIDE:
Verschachtelte NebensÃ¤tze
Passive Konstruktionen
Abstrakte Formulierungen
UnaufgelÃ¶ste AbkÃ¼rzungen
Fachsprache ohne ErklÃ¤rung
Mehrdeutige Aussagen
UnpersÃ¶nliche Formulierungen wie "Der Patient"
Meta-Kommentare Ã¼ber die Ãœbersetzung selbst
SÃ¤tze wie "Alle Angaben entsprechen dem Originaltext"
Hinweise wie "Laut Dokument" oder "GemÃ¤ÃŸ den Unterlagen"

MARKDOWN-FORMATIERUNG - SEHR WICHTIG:
â€¢ Verwende STANDARD Markdown-Listen
â€¢ Hauptpunkte: "- " (Bindestrich und Leerzeichen)
â€¢ Unterpunkte: "  - " (zwei Leerzeichen, Bindestrich, Leerzeichen)
â€¢ KEINE Bullet-Symbole (â€¢) verwenden
â€¢ Pfeile NUR in Unterpunkten: "  - â†’ Text"

RICHTIG:
- Medikament XY
  - â†’ WofÃ¼r: Senkt den Blutdruck
  - â†’ Einnahme: 1x tÃ¤glich morgens

FALSCH:
â€¢ Medikament XY. â†’ WofÃ¼r: Senkt den Blutdruck
- Medikament XY â†’ Einnahme: tÃ¤glich

EINHEITLICHES ÃœBERSETZUNGSFORMAT FÃœR ALLE DOKUMENTTYPEN:

# ðŸ“‹ Ihre medizinische Dokumentation - Einfach erklÃ¤rt

## ðŸŽ¯ Das Wichtigste zuerst
[Die zentrale Information in einem klaren Satz]

## ðŸ“Š Zusammenfassung
### Was wurde gemacht?
- [Untersuchung/Behandlung in einfacher Sprache]
- [Zeitraum/Datum wenn vorhanden]

### Was wurde gefunden?
- [Hauptbefund 1 in einfacher Sprache]
  - â†’ Bedeutung: [Was heiÃŸt das fÃ¼r Sie?]
- [Hauptbefund 2 in einfacher Sprache]
  - â†’ Bedeutung: [Was heiÃŸt das fÃ¼r Sie?]

## ðŸ¥ Ihre Diagnosen
- [Diagnose in Alltagssprache]
  - â†’ Medizinisch: [Fachbegriff]
  - â†’ ICD-Code: [Code mit ErklÃ¤rung]
  - â†’ ErklÃ¤rung: [Was ist das genau?]

## ðŸ’Š Behandlung & Medikamente
- [Medikament/Behandlung]
  - â†’ WofÃ¼r: [Zweck]
  - â†’ Einnahme: [Wie und wann]
  - â†’ Wichtig: [Besonderheiten/Nebenwirkungen]

## âœ… Ihre nÃ¤chsten Schritte
- [Was Sie tun sollen in einfacher Sprache]
- [Termine die anstehen]
- [Worauf Sie achten mÃ¼ssen in einfacher Sprache]

## ðŸ“– Fachbegriffe verstehen
- **[Begriff 1]**: [Einfache ErklÃ¤rung]
- **[Begriff 2]**: [Einfache ErklÃ¤rung]

## ðŸ”¢ Medizinische Codes erklÃ¤rt (falls vorhanden)
### ICD-Codes (Diagnose-SchlÃ¼ssel):
- **[ICD-Code]**: [VollstÃ¤ndige ErklÃ¤rung was diese Diagnose bedeutet]

### OPS-Codes (Behandlungs-SchlÃ¼ssel):
- **[OPS-Code]**: [VollstÃ¤ndige ErklÃ¤rung welche Behandlung durchgefÃ¼hrt wurde]

## âš ï¸ Wichtige Hinweise
Diese Ãœbersetzung hilft Ihnen, Ihre Unterlagen zu verstehen
Besprechen Sie alle Fragen mit Ihrem Arzt
Bei NotfÃ¤llen: 112 anrufen

---
"""

        # UNIVERSELLE Anleitung fÃ¼r ALLE medizinischen Dokumente
        universal_instruction = """
DIESES DOKUMENT KANN ENTHALTEN:
- Arztbriefe, Entlassungsbriefe, Befundberichte
- Laborwerte und Blutwerte
- Bildgebungsbefunde (RÃ¶ntgen, MRT, CT, Ultraschall)
- Pathologiebefunde
- MedikationsplÃ¤ne
- Medizinische Codes (ICD-10, OPS, DRG, GOÃ„, EBM)
- Kombinationen aus allem oben genannten

BEHANDLE JEDEN INHALT ANGEMESSEN:
- Bei Laborwerten: ErklÃ¤re Wert â†’ Normalbereich â†’ Bedeutung
- Bei Diagnosen: Ãœbersetze Fachbegriffe in Alltagssprache
- Bei Medikamenten: ErklÃ¤re Zweck und Einnahme
- Bei Bildgebung: Beschreibe was untersucht wurde und was gefunden wurde
- Bei Empfehlungen: Mache klar was der Patient tun soll
- Bei medizinischen Codes (ICD, OPS): ERKLÃ„RE immer was der Code bedeutet! Nicht nur auflisten!

  ICD-Beispiele (Diagnose-Codes):
  â€¢ "ICD I10.90" â†’ "I10.90 - Bluthochdruck ohne bekannte Ursache (Ihr Blutdruck ist dauerhaft zu hoch)"
  â€¢ "ICD E11.9" â†’ "E11.9 - Diabetes Typ 2 (Zuckerkrankheit, die meist im Erwachsenenalter auftritt)"
  â€¢ "ICD J44.0" â†’ "J44.0 - COPD mit akuter Verschlechterung (chronische Lungenerkrankung mit plÃ¶tzlicher Verschlimmerung)"
  â€¢ "ICD M54.5" â†’ "M54.5 - Kreuzschmerzen (Schmerzen im unteren RÃ¼ckenbereich)"

  OPS-Beispiele (Behandlungs-Codes):
  â€¢ "OPS 5-511.11" â†’ "5-511.11 - Entfernung der Gallenblase durch Bauchspiegelung (minimal-invasive Operation)"
  â€¢ "OPS 3-035" â†’ "3-035 - MRT des Kopfes (Kernspintomographie zur Untersuchung des Gehirns)"
  â€¢ "OPS 1-632.0" â†’ "1-632.0 - Magenspiegelung mit Gewebeentnahme (Untersuchung des Magens mit einer Kamera)"
  â€¢ "OPS 8-931.0" â†’ "8-931.0 - Ãœberwachung auf der Intensivstation (engmaschige medizinische Betreuung)"

  WICHTIG: Codes IMMER mit verstÃ¤ndlicher ErklÃ¤rung versehen! Der Patient muss verstehen, was gemeint ist!

Nutze IMMER das einheitliche Format oben, egal welche Inhalte das Dokument hat."""

        return base_instruction + universal_instruction

    def _evaluate_translation_quality(self, original: str, translated: str) -> float:
        """Evaluate the quality of the translation"""
        if not translated or translated.startswith("Error"):
            return 0.0

        confidence = 0.6  # Base confidence for OVH model

        # Length check
        if len(translated) > 100:
            confidence += 0.1
        if len(translated) > 500:
            confidence += 0.1

        # Ratio check
        length_ratio = len(translated) / max(len(original), 1)
        if 0.5 <= length_ratio <= 2.0:
            confidence += 0.1

        # Simple language indicators
        simple_indicators = [
            "this means",
            "simply put",
            "in other words",
            "das bedeutet",
            "einfach gesagt",
            "mit anderen worten",
        ]
        translated_lower = translated.lower()
        found_indicators = sum(
            1 for indicator in simple_indicators if indicator in translated_lower
        )
        confidence += min(found_indicators * 0.05, 0.1)

        return min(confidence, 1.0)

    def _improve_formatting(self, text: str) -> str:
        """
        Minimale Formatierung - konvertiert Bullet Points zu Standard Markdown
        """
        import re

        # Ersetze alle Bullet-Symbole (â€¢) durch Standard Markdown (-)
        text = re.sub(r"^â€¢", "-", text, flags=re.MULTILINE)
        text = re.sub(r"\nâ€¢", "\n-", text)

        # Stelle sicher dass Unterpunkte korrekt formatiert sind
        text = re.sub(r"^  â†’", "  - â†’", text, flags=re.MULTILINE)

        # Entferne mehrfache Leerzeilen
        text = re.sub(r"\n{3,}", "\n\n", text)

        return text.strip()

    async def format_text(self, text: str, formatting_prompt: str) -> str:
        """
        Format text using AI with a specific formatting prompt.
        This method applies document-specific formatting rules.
        """
        try:
            if not text or not formatting_prompt:
                return text

            # Create full prompt for formatting
            full_prompt = f"{formatting_prompt}\n\nTEXT TO FORMAT:\n{text}"

            # Use the medical text processing with the formatting prompt (use fast model for formatting)
            formatted_text = await self.process_medical_text_with_prompt(
                full_prompt=full_prompt,
                temperature=0.3,
                max_tokens=4000,
                use_fast_model=True,  # Formatting is routine task - use fast model
            )

            # Apply additional formatting improvements
            return self._improve_formatting(formatted_text)

        except Exception as e:
            logger.error(f"Error formatting text: {e}")
            # Return original text with basic formatting if AI formatting fails
            return self._improve_formatting(text)

    async def extract_text_with_vision(
        self,
        image_data: bytes | Image.Image,
        file_type: str = "image",
        confidence_threshold: float = 0.7,
    ) -> tuple[str, float]:
        """DEPRECATED: Vision OCR has been removed.

        Use OCREngineManager instead with:
        - Mistral OCR (primary)
        - PaddleOCR Hetzner (fallback)

        Example:
            >>> from app.services.ocr_engine_manager import OCREngineManager
            >>> manager = OCREngineManager(db_session)
            >>> result = await manager.extract_text(file_content, file_type, filename)
        """
        logger.warning("âš ï¸ DEPRECATED: extract_text_with_vision called - use OCREngineManager instead")
        return "DEPRECATED: Vision OCR removed. Use OCREngineManager with Mistral OCR or PaddleOCR.", 0.0

    async def _deprecated_extract_text_with_vision(
        self,
        image_data: bytes | Image.Image,
        file_type: str = "image",
        confidence_threshold: float = 0.7,
    ) -> tuple[str, float]:
        """DEPRECATED: Vision OCR removed - use OCREngineManager."""
        return "DEPRECATED: Vision OCR removed", 0.0

    def _get_medical_ocr_prompt(self) -> str:
        """
        Get simplified and more direct OCR prompt for medical documents
        """
        return """Extract ALL visible text from this medical document image with perfect structure preservation.

**CRITICAL TABLE FORMATTING RULES:**
- If you see a table, format it using pipe separators: | Column1 | Column2 | Column3 |
- Each table row must be on its own line
- Align columns consistently
- Preserve headers and data relationships
- Example table format:
  | Parameter | Wert | Referenzbereich | Einheit |
  | Glukose | 95 | 70-110 | mg/dl |
  | HÃ¤moglobin | 14.2 | 12.0-16.0 | g/dl |

**TEXT FORMATTING RULES:**
- Keep paragraph structure with proper line breaks
- Preserve headers, subheaders, and section divisions
- Maintain list formatting (bullet points, numbers)
- Keep date/time formatting intact
- Preserve medical terminology exactly as written

**QUALITY REQUIREMENTS:**
- Extract every visible character, number, and symbol
- Include all medical values, units, and reference ranges
- Mark unclear text as [unclear] but try to read everything
- Do NOT add extra line breaks between table rows
- Do NOT split table cells across multiple lines
- Do NOT interpret or change medical terms

Begin text extraction with perfect structure preservation:"""

    def _calculate_vision_ocr_confidence(self, text: str) -> float:
        """
        Calculate confidence score for vision OCR results
        """
        if not text or len(text.strip()) < 10:
            return 0.0

        confidence = 0.7  # Base confidence for Qwen 2.5 VL

        # Length indicators
        if len(text) > 100:
            confidence += 0.05
        if len(text) > 500:
            confidence += 0.05
        if len(text) > 1000:
            confidence += 0.05

        # Medical content indicators
        medical_terms = [
            "laborwerte",
            "befund",
            "diagnose",
            "patient",
            "arzt",
            "blutbild",
            "urin",
            "rÃ¶ntgen",
            "mrt",
            "ct",
            "ultraschall",
            "mg/dl",
            "mmol/l",
            "Âµg/ml",
            "ng/ml",
            "u/l",
            "iu/l",
            "normal",
            "pathologisch",
            "auffÃ¤llig",
            "unauffÃ¤llig",
        ]

        text_lower = text.lower()
        found_terms = sum(1 for term in medical_terms if term in text_lower)
        confidence += min(found_terms * 0.01, 0.1)

        # Structure indicators (tables, lists)
        import re

        # Reward proper table formatting with pipes
        table_rows = len(re.findall(r"\|.*\|.*\|", text))
        if table_rows >= 2:  # At least header + one data row
            confidence += min(table_rows * 0.01, 0.1)  # Up to 0.1 bonus for well-formatted tables
            logger.debug(
                f"ðŸ“Š Found {table_rows} properly formatted table rows, confidence boost: +{min(table_rows * 0.01, 0.1):.3f}"
            )

        # Basic structure indicators
        if "|" in text or "- " in text or "## " in text:
            confidence += 0.02

        # Number indicators (likely measurements) - more weight for medical values
        numbers_with_units = len(
            re.findall(
                r"\d+[.,]?\d*\s*(mg/dl|mmol/l|Âµg/l|ng/ml|u/l|iu/l|g/l|%|/Âµl)", text, re.IGNORECASE
            )
        )
        confidence += min(numbers_with_units * 0.008, 0.08)  # Higher reward for medical units

        # Penalize weird line breaks (common OCR issue)
        excessive_breaks = len(re.findall(r"\n\s*\n\s*\n", text))  # 3+ consecutive line breaks
        confidence -= min(excessive_breaks * 0.02, 0.05)  # Penalty for formatting issues

        return min(confidence, 0.95)  # Cap at 95%

    async def process_multiple_images_ocr(
        self, images: list[bytes | Image.Image], merge_strategy: str = "sequential"
    ) -> tuple[str, float]:
        """DEPRECATED: Vision OCR has been removed.

        Use OCREngineManager instead with:
        - Mistral OCR (primary)
        - PaddleOCR Hetzner (fallback)

        Example:
            >>> from app.services.ocr_engine_manager import OCREngineManager
            >>> manager = OCREngineManager(db_session)
            >>> result = await manager.extract_text(file_content, file_type, filename)
        """
        logger.warning("âš ï¸ DEPRECATED: process_multiple_images_ocr called - use OCREngineManager")
        return "DEPRECATED: Vision OCR removed. Use OCREngineManager with Mistral OCR or PaddleOCR.", 0.0

    async def _deprecated_process_multiple_images_ocr(
        self, images: list[bytes | Image.Image], merge_strategy: str = "sequential"
    ) -> tuple[str, float]:
        """DEPRECATED: Original implementation kept for reference only."""
        if not images:
            return "No images provided", 0.0

        logger.info(f"ðŸ”„ Processing {len(images)} images with vision OCR")

        ocr_results = []
        total_confidence = 0.0

        # Process all images in parallel for much faster performance
        import asyncio

        logger.info("ðŸš€ Starting parallel OCR processing for all images")

        # Limit concurrent API calls to prevent overwhelming OVH servers
        semaphore = asyncio.Semaphore(2)  # Max 2 concurrent vision API calls for stability

        async def process_single_image(i: int, image: bytes | Image.Image) -> dict:
            """Process a single image with retry logic and concurrency control"""
            async with semaphore:  # Limit concurrent API calls
                logger.info(f"ðŸ“„ Processing image {i}/{len(images)} in parallel")

                # Retry logic for failed OCR calls
                max_retries = 2
                retry_delay = 2  # seconds

                for attempt in range(max_retries + 1):
                    try:
                        text, confidence = await self.extract_text_with_vision(image, f"image_{i}")

                        # Check for successful extraction
                        if (
                            text
                            and len(text.strip()) > 20
                            and not text.startswith("Vision OCR error")
                        ):
                            logger.info(
                                f"âœ… Image {i} processed: {len(text)} chars, confidence: {confidence:.2%}"
                            )
                            return {
                                "page": i,
                                "text": text,
                                "confidence": confidence,
                                "success": True,
                            }
                        if attempt < max_retries:
                            # Failed but can retry
                            logger.warning(
                                f"âš ï¸ Image {i} attempt {attempt + 1} failed: {text[:100]}... - retrying in {retry_delay}s"
                            )
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 1.5  # Exponential backoff
                        else:
                            # Final attempt failed
                            logger.error(
                                f"âŒ Image {i} failed after {max_retries + 1} attempts: {text}"
                            )
                            return {
                                "page": i,
                                "text": text
                                or f"Vision OCR failed after {max_retries + 1} attempts",
                                "confidence": 0.0,
                                "success": False,
                            }

                    except Exception as e:
                        if attempt < max_retries:
                            logger.warning(
                                f"âš ï¸ Image {i} attempt {attempt + 1} exception: {e} - retrying in {retry_delay}s"
                            )
                            await asyncio.sleep(retry_delay)
                            retry_delay *= 1.5
                        else:
                            logger.error(
                                f"âŒ Image {i} failed after {max_retries + 1} attempts with exception: {e}"
                            )
                            return {
                                "page": i,
                                "text": f"Vision OCR exception after {max_retries + 1} attempts: {e}",
                                "confidence": 0.0,
                                "success": False,
                            }
                return None

        # Create tasks for all images
        tasks = [process_single_image(i, image) for i, image in enumerate(images, 1)]

        # Process all images concurrently
        parallel_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results - include both successful and failed pages for complete coverage
        failed_pages = []
        for result in parallel_results:
            if isinstance(result, Exception):
                logger.error(f"âŒ Parallel OCR task failed: {result}")
                continue

            if result["success"]:
                ocr_results.append(
                    {
                        "page": result["page"],
                        "text": result["text"],
                        "confidence": result["confidence"],
                    }
                )
                total_confidence += result["confidence"]
            else:
                # Keep track of failed pages for reporting
                failed_pages.append(result["page"])
                # Add placeholder text for failed pages to maintain page order
                ocr_results.append(
                    {
                        "page": result["page"],
                        "text": f"[ERROR: Page {result['page']} extraction failed - {result['text']}]",
                        "confidence": 0.0,
                    }
                )

        if not ocr_results:
            return "Failed to extract text from any image", 0.0

        # Log partial failures
        if failed_pages:
            logger.warning(f"âš ï¸ Failed to extract from {len(failed_pages)} pages: {failed_pages}")
            logger.warning(
                f"âœ… Successfully extracted from {len(ocr_results) - len(failed_pages)} pages"
            )

        # Merge results based on strategy
        if merge_strategy == "sequential":
            merged_text = self._merge_sequential(ocr_results)
        elif merge_strategy == "smart":
            merged_text = self._merge_smart(ocr_results)
        else:
            merged_text = self._merge_sequential(ocr_results)

        avg_confidence = total_confidence / len(ocr_results)

        logger.info(
            f"ðŸŽ¯ Multi-image OCR complete: {len(merged_text)} total chars, avg confidence: {avg_confidence:.2%}"
        )

        return merged_text, avg_confidence

    def _merge_sequential(self, ocr_results: list[dict]) -> str:
        """
        Merge OCR results in sequential order with page separators
        """
        merged_parts = []

        for result in sorted(ocr_results, key=lambda x: x["page"]):
            page_text = result["text"]
            page_num = result["page"]

            # Add page header
            merged_parts.append(f"--- Seite {page_num} ---")
            merged_parts.append(page_text)
            merged_parts.append("")  # Empty line between pages

        return "\n".join(merged_parts)

    def _merge_smart(self, ocr_results: list[dict]) -> str:
        """
        Intelligently merge OCR results with context awareness
        """
        if len(ocr_results) == 1:
            return ocr_results[0]["text"]

        merged_parts = []

        for i, result in enumerate(sorted(ocr_results, key=lambda x: x["page"])):
            page_text = result["text"].strip()

            if i == 0:
                # First page - add as is
                merged_parts.append(page_text)
            else:
                # Subsequent pages - check for continuation
                prev_text = merged_parts[-1] if merged_parts else ""

                # Simple heuristic: if previous page ends mid-sentence, continue
                if prev_text.rstrip().endswith((",", "-", "und", "oder", "sowie")):
                    # Likely continuation - merge without page break
                    merged_parts.append(page_text)
                else:
                    # New section - add page separator
                    merged_parts.append(f"\n--- Seite {result['page']} ---\n")
                    merged_parts.append(page_text)

        return "\n".join(merged_parts)
